{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40198003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.0.0 in /home/vscode/.local/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /home/vscode/.local/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (2.3.4)\n",
      "Requirement already satisfied: yfinance>=0.2.28 in /home/vscode/.local/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.2.66)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/vscode/.local/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (2.32.5)\n",
      "Collecting pyarrow>=14.0.0 (from -r requirements.txt (line 6))\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting lightgbm>=4.0.0 (from -r requirements.txt (line 9))\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Collecting xgboost>=2.0.0 (from -r requirements.txt (line 10))\n",
      "  Downloading xgboost-3.1.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting scikit-learn>=1.3.0 (from -r requirements.txt (line 11))\n",
      "  Downloading scikit_learn-1.7.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting catboost>=1.2.0 (from -r requirements.txt (line 12))\n",
      "  Downloading catboost-1.2.8-cp313-cp313-manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting tensorflow>=2.13.0 (from -r requirements.txt (line 15))\n",
      "  Downloading tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting ta-lib (from -r requirements.txt (line 20))\n",
      "  Downloading ta_lib-0.6.8-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
      "Collecting pandas-ta>=0.3.14b (from -r requirements.txt (line 21))\n",
      "  Downloading pandas_ta-0.4.71b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mlflow>=2.8.0 (from -r requirements.txt (line 24))\n",
      "  Downloading mlflow-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting optuna>=3.4.0 (from -r requirements.txt (line 25))\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting great-expectations>=0.18.0 (from -r requirements.txt (line 28))\n",
      "  Downloading great_expectations-1.9.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting pandera>=0.17.0 (from -r requirements.txt (line 29))\n",
      "  Downloading pandera-0.26.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting joblib>=1.3.0 (from -r requirements.txt (line 32))\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting tqdm>=4.66.0 (from -r requirements.txt (line 33))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting python-dotenv>=1.0.0 (from -r requirements.txt (line 34))\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vscode/.local/lib/python3.13/site-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vscode/.local/lib/python3.13/site-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vscode/.local/lib/python3.13/site-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /home/vscode/.local/lib/python3.13/site-packages (from yfinance>=0.2.28->-r requirements.txt (line 4)) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/vscode/.local/lib/python3.13/site-packages (from yfinance>=0.2.28->-r requirements.txt (line 4)) (4.5.0)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /home/vscode/.local/lib/python3.13/site-packages (from yfinance>=0.2.28->-r requirements.txt (line 4)) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /home/vscode/.local/lib/python3.13/site-packages (from yfinance>=0.2.28->-r requirements.txt (line 4)) (3.18.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/vscode/.local/lib/python3.13/site-packages (from yfinance>=0.2.28->-r requirements.txt (line 4)) (4.14.2)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /home/vscode/.local/lib/python3.13/site-packages (from yfinance>=0.2.28->-r requirements.txt (line 4)) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /home/vscode/.local/lib/python3.13/site-packages (from yfinance>=0.2.28->-r requirements.txt (line 4)) (6.33.0)\n",
      "Requirement already satisfied: websockets>=13.0 in /home/vscode/.local/lib/python3.13/site-packages (from yfinance>=0.2.28->-r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vscode/.local/lib/python3.13/site-packages (from requests>=2.31.0->-r requirements.txt (line 5)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.13/site-packages (from requests>=2.31.0->-r requirements.txt (line 5)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.13/site-packages (from requests>=2.31.0->-r requirements.txt (line 5)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.13/site-packages (from requests>=2.31.0->-r requirements.txt (line 5)) (2025.10.5)\n",
      "Collecting scipy (from lightgbm>=4.0.0->-r requirements.txt (line 9))\n",
      "  Downloading scipy-1.16.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost>=2.0.0->-r requirements.txt (line 10))\n",
      "  Downloading nvidia_nccl_cu12-2.28.7-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.3.0->-r requirements.txt (line 11))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting graphviz (from catboost>=1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting matplotlib (from catboost>=1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading matplotlib-3.10.7-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting plotly (from catboost>=1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading plotly-6.4.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: six in /home/vscode/.local/lib/python3.13/site-packages (from catboost>=1.2.0->-r requirements.txt (line 12)) (1.17.0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/vscode/.local/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (25.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (78.1.1)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/vscode/.local/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 15)) (4.15.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading wrapt-2.0.1-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading h5py-3.15.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading ml_dtypes-0.5.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pillow (from tensorboard~=2.20.0->tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading pillow-12.0.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting build (from ta-lib->-r requirements.txt (line 20))\n",
      "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting numba==0.61.2 (from pandas-ta>=0.3.14b->-r requirements.txt (line 21))\n",
      "  Downloading numba-0.61.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->pandas-ta>=0.3.14b->-r requirements.txt (line 21))\n",
      "  Downloading llvmlite-0.44.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting numpy>=1.24.0 (from -r requirements.txt (line 3))\n",
      "  Downloading numpy-2.2.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting mlflow-skinny==3.6.0 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading mlflow_skinny-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting mlflow-tracing==3.6.0 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading mlflow_tracing-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask-CORS<7 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Flask<4 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading alembic-1.17.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting cryptography<47,>=43.0.0 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting huey<3,>=2.5.0 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading huey-2.5.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading sqlalchemy-2.0.44-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading cachetools-6.2.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting click<9,>=7.0 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading databricks_sdk-0.73.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading fastapi-0.121.1-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.13/site-packages (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24)) (3.1.41)\n",
      "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pydantic<3,>=2.0.0 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /home/vscode/.local/lib/python3.13/site-packages (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24)) (6.0.3)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /home/vscode/.local/lib/python3.13/site-packages (from cryptography<47,>=43.0.0->mlflow>=2.8.0->-r requirements.txt (line 24)) (2.0.0)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting starlette<0.50.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading starlette-0.49.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading annotated_doc-0.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /home/vscode/.local/lib/python3.13/site-packages (from Flask<4->mlflow>=2.8.0->-r requirements.txt (line 24)) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /home/vscode/.local/lib/python3.13/site-packages (from Flask<4->mlflow>=2.8.0->-r requirements.txt (line 24)) (3.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.13/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.13/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24)) (5.0.2)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->catboost>=1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->catboost>=1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->catboost>=1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading fonttools-4.60.1-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->catboost>=1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading kiwisolver-1.4.9-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib->catboost>=1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow>=2.8.0->-r requirements.txt (line 24))\n",
      "  Downloading greenlet-3.2.4-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/vscode/.local/lib/python3.13/site-packages (from starlette<0.50.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24)) (4.11.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/vscode/.local/lib/python3.13/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24)) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/vscode/.local/lib/python3.13/site-packages (from uvicorn<1->mlflow-skinny==3.6.0->mlflow>=2.8.0->-r requirements.txt (line 24)) (0.16.0)\n",
      "Collecting colorlog (from optuna>=3.4.0->-r requirements.txt (line 25))\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting altair<5.0.0,>=4.2.1 (from great-expectations>=0.18.0->-r requirements.txt (line 28))\n",
      "  Downloading altair-4.2.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /home/vscode/.local/lib/python3.13/site-packages (from great-expectations>=0.18.0->-r requirements.txt (line 28)) (4.25.1)\n",
      "Collecting marshmallow<4.0.0,>=3.7.1 (from great-expectations>=0.18.0->-r requirements.txt (line 28))\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: mistune>=0.8.4 in /home/vscode/.local/lib/python3.13/site-packages (from great-expectations>=0.18.0->-r requirements.txt (line 28)) (3.1.4)\n",
      "Collecting ruamel.yaml>=0.16 (from great-expectations>=0.18.0->-r requirements.txt (line 28))\n",
      "  Downloading ruamel.yaml-0.18.16-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tzlocal>=1.2 (from great-expectations>=0.18.0->-r requirements.txt (line 28))\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting entrypoints (from altair<5.0.0,>=4.2.1->great-expectations>=0.18.0->-r requirements.txt (line 28))\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting toolz (from altair<5.0.0,>=4.2.1->great-expectations>=0.18.0->-r requirements.txt (line 28))\n",
      "  Downloading toolz-1.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting typeguard (from pandera>=0.17.0->-r requirements.txt (line 29))\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing_inspect>=0.6.0 (from pandera>=0.17.0->-r requirements.txt (line 29))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/vscode/.local/lib/python3.13/site-packages (from beautifulsoup4>=4.11.1->yfinance>=0.2.28->-r requirements.txt (line 4)) (2.8)\n",
      "Requirement already satisfied: pycparser in /home/vscode/.local/lib/python3.13/site-packages (from cffi>=2.0.0->cryptography<47,>=43.0.0->mlflow>=2.8.0->-r requirements.txt (line 24)) (2.23)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/vscode/.local/lib/python3.13/site-packages (from jsonschema>=2.5.1->great-expectations>=0.18.0->-r requirements.txt (line 28)) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/vscode/.local/lib/python3.13/site-packages (from jsonschema>=2.5.1->great-expectations>=0.18.0->-r requirements.txt (line 28)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/vscode/.local/lib/python3.13/site-packages (from jsonschema>=2.5.1->great-expectations>=0.18.0->-r requirements.txt (line 28)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/vscode/.local/lib/python3.13/site-packages (from jsonschema>=2.5.1->great-expectations>=0.18.0->-r requirements.txt (line 28)) (0.28.0)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading optree-0.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.16->great-expectations>=0.18.0->-r requirements.txt (line 28))\n",
      "  Downloading ruamel.yaml.clib-0.2.14-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing_inspect>=0.6.0->pandera>=0.17.0->-r requirements.txt (line 29))\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyproject_hooks (from build->ta-lib->-r requirements.txt (line 20))\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost>=1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading narwhals-2.10.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vscode/.local/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 15)) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 15))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xgboost-3.1.1-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading catboost-1.2.8-cp313-cp313-manylinux2014_x86_64.whl (99.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.8/620.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m  \u001b[33m0:00:13\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ta_lib-0.6.8-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas_ta-0.4.71b0-py3-none-any.whl (240 kB)\n",
      "Downloading numba-0.61.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mlflow-3.6.0-py3-none-any.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-3.6.0-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_tracing-3.6.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading alembic-1.17.1-py3-none-any.whl (247 kB)\n",
      "Downloading cachetools-6.2.1-py3-none-any.whl (11 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading databricks_sdk-0.73.0-py3-none-any.whl (753 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.9/753.9 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading fastapi-0.121.1-py3-none-any.whl (109 kB)\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Downloading huey-2.5.4-py3-none-any.whl (76 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading matplotlib-3.10.7-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading scipy-1.16.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.44-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Downloading starlette-0.49.3-py3-none-any.whl (74 kB)\n",
      "Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "Downloading great_expectations-1.9.0-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pandera-0.26.1-py3-none-any.whl (292 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading annotated_doc-0.0.3-py3-none-any.whl (5.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading contourpy-1.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading fonttools-4.60.1-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading greenlet-3.2.4-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (610 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m610.5/610.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.15.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading pillow-12.0.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading ruamel.yaml-0.18.16-py3-none-any.whl (119 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.14-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (744 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m744.1/744.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wrapt-2.0.1-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading nvidia_nccl_cu12-2.28.7-py3-none-manylinux_2_18_x86_64.whl (296.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.8/296.8 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading optree-0.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (414 kB)\n",
      "Downloading plotly-6.4.0-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.10.2-py3-none-any.whl (419 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading toolz-1.1.0-py3-none-any.whl (58 kB)\n",
      "Downloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: namex, libclang, huey, flatbuffers, zipp, wrapt, wheel, werkzeug, tzlocal, typing-inspection, typeguard, tqdm, toolz, threadpoolctl, termcolor, tensorboard-data-server, sqlparse, ruamel.yaml.clib, python-dotenv, pyproject_hooks, pyparsing, pydantic-core, pyasn1, pyarrow, pillow, optree, opt_einsum, opentelemetry-proto, nvidia-nccl-cu12, numpy, narwhals, mypy-extensions, mdurl, marshmallow, markdown, Mako, llvmlite, kiwisolver, joblib, itsdangerous, gunicorn, grpcio, greenlet, graphviz, graphql-core, google_pasta, gast, fonttools, entrypoints, cycler, colorlog, cloudpickle, click, cachetools, blinker, annotated-types, annotated-doc, absl-py, uvicorn, typing_inspect, tensorboard, starlette, sqlalchemy, scipy, ruamel.yaml, rsa, pydantic, pyasn1-modules, plotly, numba, ml_dtypes, markdown-it-py, importlib_metadata, h5py, graphql-relay, Flask, docker, cryptography, contourpy, build, astunparse, xgboost, ta-lib, scikit-learn, rich, pandera, pandas-ta, opentelemetry-api, matplotlib, lightgbm, graphene, google-auth, Flask-CORS, fastapi, alembic, optuna, opentelemetry-semantic-conventions, keras, databricks-sdk, catboost, altair, tensorflow, opentelemetry-sdk, great-expectations, mlflow-tracing, mlflow-skinny, mlflow\n",
      "\u001b[2K  Attempting uninstall: numpy[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 28/107\u001b[0m [nvidia-nccl-cu12]server]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.4━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 28/107\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling numpy-2.3.4:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 29/107\u001b[0m [numpy]l-cu12]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.4━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 29/107\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107/107\u001b[0m [mlflow] [mlflow]skinny]w-tracing]ons]entions]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Flask-3.1.2 Flask-CORS-6.0.1 Mako-1.3.10 absl-py-2.3.1 alembic-1.17.1 altair-4.2.2 annotated-doc-0.0.3 annotated-types-0.7.0 astunparse-1.6.3 blinker-1.9.0 build-1.3.0 cachetools-6.2.1 catboost-1.2.8 click-8.3.0 cloudpickle-3.1.2 colorlog-6.10.1 contourpy-1.3.3 cryptography-46.0.3 cycler-0.12.1 databricks-sdk-0.73.0 docker-7.1.0 entrypoints-0.4 fastapi-0.121.1 flatbuffers-25.9.23 fonttools-4.60.1 gast-0.6.0 google-auth-2.43.0 google_pasta-0.2.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 graphviz-0.21 great-expectations-1.9.0 greenlet-3.2.4 grpcio-1.76.0 gunicorn-23.0.0 h5py-3.15.1 huey-2.5.4 importlib_metadata-8.7.0 itsdangerous-2.2.0 joblib-1.5.2 keras-3.12.0 kiwisolver-1.4.9 libclang-18.1.1 lightgbm-4.6.0 llvmlite-0.44.0 markdown-3.10 markdown-it-py-4.0.0 marshmallow-3.26.1 matplotlib-3.10.7 mdurl-0.1.2 ml_dtypes-0.5.3 mlflow-3.6.0 mlflow-skinny-3.6.0 mlflow-tracing-3.6.0 mypy-extensions-1.1.0 namex-0.1.0 narwhals-2.10.2 numba-0.61.2 numpy-2.2.6 nvidia-nccl-cu12-2.28.7 opentelemetry-api-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 opt_einsum-3.4.0 optree-0.17.0 optuna-4.5.0 pandas-ta-0.4.71b0 pandera-0.26.1 pillow-12.0.0 plotly-6.4.0 pyarrow-22.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.12.4 pydantic-core-2.41.5 pyparsing-3.2.5 pyproject_hooks-1.2.0 python-dotenv-1.2.1 rich-14.2.0 rsa-4.9.1 ruamel.yaml-0.18.16 ruamel.yaml.clib-0.2.14 scikit-learn-1.7.2 scipy-1.16.3 sqlalchemy-2.0.44 sqlparse-0.5.3 starlette-0.49.3 ta-lib-0.6.8 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 threadpoolctl-3.6.0 toolz-1.1.0 tqdm-4.67.1 typeguard-4.4.4 typing-inspection-0.4.2 typing_inspect-0.9.0 tzlocal-5.3.1 uvicorn-0.38.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-2.0.1 xgboost-3.1.1 zipp-3.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b14995",
   "metadata": {},
   "source": [
    "1.load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1fc3707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "================= LOADING 6 MONTHS OF 5-MINUTE DATA FOR TESLA ==================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fetching 6 Months of 5-Minute Bar Data\n",
      "============================================================\n",
      "\n",
      "Using fallback method (yfinance + interpolation)...\n",
      "Using fallback method (limited to 60 days of 5-min data)\n",
      "Fetching recent 60 days of 5-minute data...\n",
      "  ✅ Fetched 3267 5-minute bars\n",
      "Fetching older data at 15-minute intervals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TSLA: possibly delisted; no price data found  (15m 2025-05-13 18:48:55.159326 -> 2025-09-10 18:48:55.159326) (Yahoo error = \"15m data not available for startTime=1747176535 and endTime=1757544535. The requested range must be within the last 60 days.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully created 3267 bars via fallback method\n",
      "\n",
      "Adding market features...\n",
      "✅ Added market features\n",
      "\n",
      "Validating data...\n",
      "\n",
      "📊 Data Statistics:\n",
      "  Date range: 2025-09-11 to 2025-11-07\n",
      "  Total bars: 3,267\n",
      "  Trading days: ~57\n",
      "  Bars per day: ~57\n",
      "  Expected (78 bars/day): 4,446\n",
      "\n",
      "✅ Data saved to tesla_6months_5min_data.csv\n",
      "✅ Data saved to tesla_6months_5min_data.pkl\n",
      "\n",
      "📈 Final Dataset:\n",
      "  Shape: (3267, 11)\n",
      "  Memory usage: 0.27 MB\n",
      "  Columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'dividends', 'stock splits', 'vix', 'sentiment_score', 'transactions']\n",
      "\n",
      "🔮 For LSTM Training:\n",
      "  Lookback period: 20 days\n",
      "  Sequence length: 1560 bars\n",
      "  Potential training sequences: ~21\n",
      "  ⚠️ Warning: May need more data for robust training\n",
      "\n",
      "✅ Data ready for hyperparameter tuning!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TeslaDataCollector:\n",
    "    \"\"\"\n",
    "    Collect 6 months of 5-minute bar data for Tesla\n",
    "    Note: yfinance has limitations on historical 5-minute data\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker='TSLA'):\n",
    "        self.ticker = ticker\n",
    "        self.data_cache_dir = 'tesla_data_cache'\n",
    "        \n",
    "        # Create cache directory\n",
    "        if not os.path.exists(self.data_cache_dir):\n",
    "            os.makedirs(self.data_cache_dir)\n",
    "    \n",
    "    def fetch_5min_data_polygon(self, start_date, end_date, api_key=None):\n",
    "        \"\"\"\n",
    "        Fetch 5-minute data using Polygon.io API (recommended for 6 months)\n",
    "        Requires API key but provides reliable historical data\n",
    "        \"\"\"\n",
    "        if not api_key:\n",
    "            print(\"⚠️ Polygon API key not provided. Using fallback method...\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            from polygon import RESTClient\n",
    "            client = RESTClient(api_key)\n",
    "            \n",
    "            bars = []\n",
    "            current_date = start_date\n",
    "            \n",
    "            while current_date < end_date:\n",
    "                next_date = min(current_date + timedelta(days=30), end_date)\n",
    "                \n",
    "                print(f\"  Fetching {current_date.date()} to {next_date.date()}...\")\n",
    "                \n",
    "                aggs = client.get_aggs(\n",
    "                    ticker=self.ticker,\n",
    "                    multiplier=5,\n",
    "                    timespan=\"minute\",\n",
    "                    from_=current_date.strftime('%Y-%m-%d'),\n",
    "                    to=next_date.strftime('%Y-%m-%d'),\n",
    "                    limit=50000\n",
    "                )\n",
    "                \n",
    "                bars.extend(aggs)\n",
    "                current_date = next_date\n",
    "                time.sleep(0.2)  # Rate limiting\n",
    "            \n",
    "            df = pd.DataFrame(bars)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Polygon API error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def fetch_5min_data_alpaca(self, start_date, end_date, api_key=None, secret_key=None):\n",
    "        \"\"\"\n",
    "        Fetch 5-minute data using Alpaca API (free with account)\n",
    "        Good alternative for 6 months of 5-minute data\n",
    "        \"\"\"\n",
    "        if not api_key or not secret_key:\n",
    "            print(\"⚠️ Alpaca API credentials not provided. Using fallback method...\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            from alpaca.data.historical import StockHistoricalDataClient\n",
    "            from alpaca.data.requests import StockBarsRequest\n",
    "            from alpaca.data.timeframe import TimeFrame\n",
    "            \n",
    "            client = StockHistoricalDataClient(api_key, secret_key)\n",
    "            \n",
    "            request_params = StockBarsRequest(\n",
    "                symbol_or_symbols=self.ticker,\n",
    "                timeframe=TimeFrame.Minute,\n",
    "                start=start_date,\n",
    "                end=end_date\n",
    "            )\n",
    "            \n",
    "            bars = client.get_stock_bars(request_params)\n",
    "            df = bars.df.reset_index()\n",
    "            \n",
    "            # Resample to 5-minute bars\n",
    "            df = df.set_index('timestamp')\n",
    "            df_5min = df.resample('5T').agg({\n",
    "                'open': 'first',\n",
    "                'high': 'max',\n",
    "                'low': 'min',\n",
    "                'close': 'last',\n",
    "                'volume': 'sum',\n",
    "                'trade_count': 'sum'\n",
    "            })\n",
    "            \n",
    "            return df_5min.reset_index()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Alpaca API error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def fetch_5min_data_fallback(self, months_back=6):\n",
    "        \"\"\"\n",
    "        Fallback method: Combine multiple data sources\n",
    "        Since yfinance limits 5-minute data to 60 days\n",
    "        \"\"\"\n",
    "        print(\"Using fallback method (limited to 60 days of 5-min data)\")\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        # 1. Get last 60 days of 5-minute data from yfinance\n",
    "        print(\"Fetching recent 60 days of 5-minute data...\")\n",
    "        \n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=60)\n",
    "        \n",
    "        ticker = yf.Ticker(self.ticker)\n",
    "        df_5min = ticker.history(\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            interval='5m'\n",
    "        )\n",
    "        \n",
    "        if not df_5min.empty:\n",
    "            df_5min = df_5min.reset_index()\n",
    "            df_5min.columns = [col.lower() for col in df_5min.columns]\n",
    "            \n",
    "            if 'datetime' in df_5min.columns:\n",
    "                df_5min = df_5min.rename(columns={'datetime': 'timestamp'})\n",
    "            elif 'index' in df_5min.columns:\n",
    "                df_5min = df_5min.rename(columns={'index': 'timestamp'})\n",
    "            \n",
    "            all_data.append(df_5min)\n",
    "            print(f\"  ✅ Fetched {len(df_5min)} 5-minute bars\")\n",
    "        \n",
    "        # 2. Get older data at 15-minute intervals and interpolate\n",
    "        print(\"Fetching older data at 15-minute intervals...\")\n",
    "        \n",
    "        older_end = start_date\n",
    "        older_start = older_end - timedelta(days=120)  # 4 more months\n",
    "        \n",
    "        df_15min = ticker.history(\n",
    "            start=older_start,\n",
    "            end=older_end,\n",
    "            interval='15m'\n",
    "        )\n",
    "        \n",
    "        if not df_15min.empty:\n",
    "            df_15min = df_15min.reset_index()\n",
    "            df_15min.columns = [col.lower() for col in df_15min.columns]\n",
    "            \n",
    "            if 'datetime' in df_15min.columns:\n",
    "                df_15min = df_15min.rename(columns={'datetime': 'timestamp'})\n",
    "            elif 'index' in df_15min.columns:\n",
    "                df_15min = df_15min.rename(columns={'index': 'timestamp'})\n",
    "            \n",
    "            # Interpolate to approximate 5-minute bars\n",
    "            df_15min_resampled = self.interpolate_to_5min(df_15min)\n",
    "            all_data.append(df_15min_resampled)\n",
    "            print(f\"  ✅ Interpolated {len(df_15min_resampled)} 5-minute bars from 15-min data\")\n",
    "        \n",
    "        # Combine all data\n",
    "        if all_data:\n",
    "            df_combined = pd.concat(all_data, axis=0)\n",
    "            df_combined = df_combined.drop_duplicates(subset=['timestamp'])\n",
    "            df_combined = df_combined.sort_values('timestamp')\n",
    "            return df_combined\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def interpolate_to_5min(self, df_15min):\n",
    "        \"\"\"\n",
    "        Interpolate 15-minute bars to approximate 5-minute bars\n",
    "        This is an approximation for backtesting when real 5-min data isn't available\n",
    "        \"\"\"\n",
    "        df = df_15min.set_index('timestamp')\n",
    "        \n",
    "        # Resample to 5-minute intervals\n",
    "        df_5min = pd.DataFrame()\n",
    "        \n",
    "        # For each 15-minute bar, create three 5-minute bars\n",
    "        for idx, row in df.iterrows():\n",
    "            # Simulate intraday price movement\n",
    "            open_price = row['open']\n",
    "            close_price = row['close']\n",
    "            high_price = row['high']\n",
    "            low_price = row['low']\n",
    "            volume = row['volume']\n",
    "            \n",
    "            # Create 3 intermediate 5-minute bars\n",
    "            mid1 = open_price + (close_price - open_price) * 0.33\n",
    "            mid2 = open_price + (close_price - open_price) * 0.67\n",
    "            \n",
    "            bars = []\n",
    "            \n",
    "            # First 5-min bar\n",
    "            bars.append({\n",
    "                'timestamp': idx,\n",
    "                'open': open_price,\n",
    "                'high': max(open_price, mid1) + np.random.uniform(0, (high_price - close_price) * 0.3),\n",
    "                'low': min(open_price, mid1) - np.random.uniform(0, (open_price - low_price) * 0.3),\n",
    "                'close': mid1,\n",
    "                'volume': volume * 0.33\n",
    "            })\n",
    "            \n",
    "            # Second 5-min bar\n",
    "            bars.append({\n",
    "                'timestamp': idx + timedelta(minutes=5),\n",
    "                'open': mid1,\n",
    "                'high': max(mid1, mid2) + np.random.uniform(0, (high_price - close_price) * 0.3),\n",
    "                'low': min(mid1, mid2) - np.random.uniform(0, (open_price - low_price) * 0.3),\n",
    "                'close': mid2,\n",
    "                'volume': volume * 0.33\n",
    "            })\n",
    "            \n",
    "            # Third 5-min bar\n",
    "            bars.append({\n",
    "                'timestamp': idx + timedelta(minutes=10),\n",
    "                'open': mid2,\n",
    "                'high': max(mid2, close_price) + np.random.uniform(0, (high_price - close_price) * 0.3),\n",
    "                'low': min(mid2, close_price) - np.random.uniform(0, (open_price - low_price) * 0.3),\n",
    "                'close': close_price,\n",
    "                'volume': volume * 0.34\n",
    "            })\n",
    "            \n",
    "            df_5min = pd.concat([df_5min, pd.DataFrame(bars)])\n",
    "        \n",
    "        return df_5min.reset_index(drop=True)\n",
    "    \n",
    "    def fetch_complete_dataset(self, api_key=None, api_secret=None):\n",
    "        \"\"\"\n",
    "        Main method to fetch 6 months of 5-minute data\n",
    "        Tries multiple sources in order of preference\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Fetching 6 Months of 5-Minute Bar Data\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=180)  # 6 months\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_file = os.path.join(self.data_cache_dir, f'{self.ticker}_5min_6months.csv')\n",
    "        \n",
    "        if os.path.exists(cache_file):\n",
    "            # Check if cache is recent (less than 1 day old)\n",
    "            file_time = datetime.fromtimestamp(os.path.getmtime(cache_file))\n",
    "            if datetime.now() - file_time < timedelta(days=1):\n",
    "                print(\"📁 Loading from cache...\")\n",
    "                df = pd.read_csv(cache_file)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                print(f\"✅ Loaded {len(df)} bars from cache\")\n",
    "                return df\n",
    "        \n",
    "        # Try Polygon API first (best option)\n",
    "        if api_key and 'polygon' in api_key.lower():\n",
    "            print(\"Attempting to fetch via Polygon API...\")\n",
    "            df = self.fetch_5min_data_polygon(start_date, end_date, api_key)\n",
    "            if df is not None and not df.empty:\n",
    "                df.to_csv(cache_file, index=False)\n",
    "                print(f\"✅ Successfully fetched {len(df)} bars via Polygon\")\n",
    "                return df\n",
    "        \n",
    "        # Try Alpaca API second\n",
    "        if api_key and api_secret:\n",
    "            print(\"Attempting to fetch via Alpaca API...\")\n",
    "            df = self.fetch_5min_data_alpaca(start_date, end_date, api_key, api_secret)\n",
    "            if df is not None and not df.empty:\n",
    "                df.to_csv(cache_file, index=False)\n",
    "                print(f\"✅ Successfully fetched {len(df)} bars via Alpaca\")\n",
    "                return df\n",
    "        \n",
    "        # Use fallback method\n",
    "        print(\"Using fallback method (yfinance + interpolation)...\")\n",
    "        df = self.fetch_5min_data_fallback(months_back=6)\n",
    "        \n",
    "        if not df.empty:\n",
    "            df.to_csv(cache_file, index=False)\n",
    "            print(f\"✅ Successfully created {len(df)} bars via fallback method\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_market_features(self, df):\n",
    "        \"\"\"\n",
    "        Add VIX, sentiment, and other market features\n",
    "        \"\"\"\n",
    "        print(\"\\nAdding market features...\")\n",
    "        \n",
    "        # Fetch VIX data\n",
    "        vix = yf.Ticker(\"^VIX\")\n",
    "        end_date = df['timestamp'].max()\n",
    "        start_date = df['timestamp'].min()\n",
    "        \n",
    "        vix_data = vix.history(\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            interval='1d'  # Daily VIX is sufficient\n",
    "        )\n",
    "        \n",
    "        if not vix_data.empty:\n",
    "            # Resample VIX to match our 5-minute data\n",
    "            vix_data = vix_data['Close']\n",
    "            vix_data = vix_data.resample('5T').fillna(method='ffill')\n",
    "            \n",
    "            df = df.set_index('timestamp')\n",
    "            df['vix'] = vix_data\n",
    "            df = df.reset_index()\n",
    "        else:\n",
    "            # Default VIX value\n",
    "            df['vix'] = 20.0\n",
    "        \n",
    "        # Add sentiment score (simplified - in production use news API)\n",
    "        df['sentiment_score'] = 0.5 + 0.3 * np.sin(np.arange(len(df)) / 1000)\n",
    "        df['sentiment_score'] = np.clip(df['sentiment_score'], -1, 1)\n",
    "        \n",
    "        # Add transaction count estimate\n",
    "        df['transactions'] = (df['volume'] / 100).astype(int) + np.random.randint(50, 200, len(df))\n",
    "        \n",
    "        # Forward fill any NaN values\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        print(f\"✅ Added market features\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def validate_data(self, df):\n",
    "        \"\"\"\n",
    "        Validate and clean the data\n",
    "        \"\"\"\n",
    "        print(\"\\nValidating data...\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "        for col in required_cols:\n",
    "            if col not in df.columns:\n",
    "                print(f\"❌ Missing required column: {col}\")\n",
    "                return None\n",
    "        \n",
    "        # Remove any rows where OHLC relationship is violated\n",
    "        invalid_mask = (df['high'] < df['low']) | \\\n",
    "                      (df['high'] < df['open']) | \\\n",
    "                      (df['high'] < df['close']) | \\\n",
    "                      (df['low'] > df['open']) | \\\n",
    "                      (df['low'] > df['close'])\n",
    "        \n",
    "        if invalid_mask.any():\n",
    "            print(f\"  Removing {invalid_mask.sum()} invalid OHLC rows\")\n",
    "            df = df[~invalid_mask]\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates(subset=['timestamp'])\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        df = df.sort_values('timestamp')\n",
    "        \n",
    "        # Calculate some statistics\n",
    "        date_range = (df['timestamp'].max() - df['timestamp'].min()).days\n",
    "        bars_per_day = len(df) / max(date_range, 1)\n",
    "        \n",
    "        print(f\"\\n📊 Data Statistics:\")\n",
    "        print(f\"  Date range: {df['timestamp'].min().date()} to {df['timestamp'].max().date()}\")\n",
    "        print(f\"  Total bars: {len(df):,}\")\n",
    "        print(f\"  Trading days: ~{date_range}\")\n",
    "        print(f\"  Bars per day: ~{bars_per_day:.0f}\")\n",
    "        print(f\"  Expected (78 bars/day): {date_range * 78:,}\")\n",
    "        \n",
    "        if len(df) < date_range * 50:  # Less than 50 bars per day suggests missing data\n",
    "            print(\"  ⚠️ Warning: Data might be incomplete\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Main function to load data\n",
    "def load_6months_5min_data(api_key=None, api_secret=None):\n",
    "    \"\"\"\n",
    "    Main function to load 6 months of 5-minute bar data\n",
    "    \n",
    "    Args:\n",
    "        api_key: Optional API key for Polygon or Alpaca\n",
    "        api_secret: Optional API secret for Alpaca\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 6 months of 5-minute bars\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" LOADING 6 MONTHS OF 5-MINUTE DATA FOR TESLA \".center(80, \"=\"))\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Initialize collector\n",
    "    collector = TeslaDataCollector('TSLA')\n",
    "    \n",
    "    # Fetch the data\n",
    "    df = collector.fetch_complete_dataset(api_key, api_secret)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"❌ Failed to fetch data\")\n",
    "        return None\n",
    "    \n",
    "    # Add market features\n",
    "    df = collector.add_market_features(df)\n",
    "    \n",
    "    # Validate data\n",
    "    df = collector.validate_data(df)\n",
    "    \n",
    "    if df is not None:\n",
    "        # Save the final dataset\n",
    "        output_file = 'tesla_6months_5min_data.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\n✅ Data saved to {output_file}\")\n",
    "        \n",
    "        # Also save as pickle for faster loading\n",
    "        df.to_pickle('tesla_6months_5min_data.pkl')\n",
    "        print(f\"✅ Data saved to tesla_6months_5min_data.pkl\")\n",
    "        \n",
    "        print(f\"\\n📈 Final Dataset:\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "        print(f\"  Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Calculate expected training sequences\n",
    "        lookback_days = 20\n",
    "        bars_per_day = 78\n",
    "        sequence_length = lookback_days * bars_per_day\n",
    "        num_sequences = (len(df) - sequence_length) // bars_per_day\n",
    "        \n",
    "        print(f\"\\n🔮 For LSTM Training:\")\n",
    "        print(f\"  Lookback period: {lookback_days} days\")\n",
    "        print(f\"  Sequence length: {sequence_length} bars\")\n",
    "        print(f\"  Potential training sequences: ~{num_sequences}\")\n",
    "        \n",
    "        if num_sequences < 100:\n",
    "            print(\"  ⚠️ Warning: May need more data for robust training\")\n",
    "        else:\n",
    "            print(\"  ✅ Sufficient sequences for training\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Quick loader function for subsequent use\n",
    "def quick_load():\n",
    "    \"\"\"\n",
    "    Quick loader for already downloaded data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try pickle first (faster)\n",
    "        df = pd.read_pickle('tesla_6months_5min_data.pkl')\n",
    "        print(f\"✅ Loaded {len(df)} bars from pickle file\")\n",
    "        return df\n",
    "    except:\n",
    "        try:\n",
    "            # Fall back to CSV\n",
    "            df = pd.read_csv('tesla_6months_5min_data.csv')\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            print(f\"✅ Loaded {len(df)} bars from CSV file\")\n",
    "            return df\n",
    "        except:\n",
    "            print(\"❌ No saved data found. Running full data collection...\")\n",
    "            return load_6months_5min_data()\n",
    "\n",
    "# Usage examples\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Option 1: Load without API (uses fallback method)\n",
    "    df = load_6months_5min_data()\n",
    "    \n",
    "    # Option 2: Load with Polygon API (best quality)\n",
    "    # df = load_6months_5min_data(api_key='your_polygon_api_key')\n",
    "    \n",
    "    # Option 3: Load with Alpaca API (free with account)\n",
    "    # df = load_6months_5min_data(\n",
    "    #     api_key='your_alpaca_api_key',\n",
    "    #     api_secret='your_alpaca_secret_key'\n",
    "    # )\n",
    "    \n",
    "    # Option 4: Quick load if data already exists\n",
    "    # df = quick_load()\n",
    "    \n",
    "    if df is not None:\n",
    "        print(\"\\n✅ Data ready for hyperparameter tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05932c66",
   "metadata": {},
   "source": [
    "# Just run this to get your data:\n",
    "df = load_6months_5min_data()\n",
    "\n",
    "# Data is automatically saved to:\n",
    "# - tesla_6months_5min_data.csv\n",
    "# - tesla_6months_5min_data.pkl\n",
    "\n",
    "# For subsequent runs, use quick loader:\n",
    "df = quick_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99467ed9",
   "metadata": {},
   "source": [
    "2.hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af575ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:05:02.661207: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-09 19:05:24.029431: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-09 19:05:35.920548: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "=============== HYPERPARAMETER TUNING FOR TESLA OHLC PREDICTION ================\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "✅ Loaded 3267 bars from saved data\n",
      "\n",
      "============================================================\n",
      "Advanced Hyperparameter Tuning\n",
      "============================================================\n",
      "\n",
      "Creating features...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True, at position 2877",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 620\u001b[39m\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m run_hyperparameter_tuning(n_trials=n_trials)\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    619\u001b[39m     \u001b[38;5;66;03m# Run full tuning with 100 trials\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     best_params = \u001b[43mrun_hyperparameter_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# Or run quick test\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;66;03m# best_params = quick_test_tuning()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 599\u001b[39m, in \u001b[36mrun_hyperparameter_tuning\u001b[39m\u001b[34m(df, n_trials)\u001b[39m\n\u001b[32m    593\u001b[39m tuner = AdvancedHyperparameterTuner(\n\u001b[32m    594\u001b[39m     lookback_days=\u001b[32m20\u001b[39m,\n\u001b[32m    595\u001b[39m     interval_minutes=\u001b[32m5\u001b[39m\n\u001b[32m    596\u001b[39m )\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Run tuning\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m best_params = \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_params:\n\u001b[32m    602\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 348\u001b[39m, in \u001b[36mAdvancedHyperparameterTuner.tune\u001b[39m\u001b[34m(self, df, n_trials, n_jobs)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# Prepare features\u001b[39;00m\n\u001b[32m    347\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeatures shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# Prepare sequences\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mAdvancedHyperparameterTuner.create_features\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Time-based features (important for intraday patterns)\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     features[\u001b[33m'\u001b[39m\u001b[33mhour\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.dt.hour\n\u001b[32m     89\u001b[39m     features[\u001b[33m'\u001b[39m\u001b[33mminute\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m]).dt.minute\n\u001b[32m     90\u001b[39m     features[\u001b[33m'\u001b[39m\u001b[33mday_of_week\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m]).dt.dayofweek\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/core/tools/datetimes.py:1072\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1070\u001b[39m         result = arg.map(cache_array)\n\u001b[32m   1071\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m         values = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m         result = arg._constructor(values, index=arg.index, name=arg.name)\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc.MutableMapping)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/core/tools/datetimes.py:437\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m result, tz_parsed = \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[32m    449\u001b[39m     out_unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/core/arrays/datetimes.py:2415\u001b[39m, in \u001b[36mobjects_to_datetime64\u001b[39m\u001b[34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[39m\n\u001b[32m   2412\u001b[39m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[32m   2413\u001b[39m data = np.asarray(data, dtype=np.object_)\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m result, tz_parsed = \u001b[43mtslib\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2417\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[43m=\u001b[49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2425\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m   2426\u001b[39m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[32m   2427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:412\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:596\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:504\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/strptime.pyx:285\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.DatetimeParseState.process_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True, at position 2877"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AdvancedHyperparameterTuner:\n",
    "    \"\"\"\n",
    "    Advanced hyperparameter tuning for OHLC prediction with 6 months of data\n",
    "    \"\"\"\n",
    "    def __init__(self, lookback_days=20, interval_minutes=5):\n",
    "        self.lookback_days = lookback_days\n",
    "        self.interval_minutes = interval_minutes\n",
    "        self.bars_per_day = 390 // interval_minutes  # 78 for 5-min bars\n",
    "        self.sequence_length = self.bars_per_day * lookback_days\n",
    "        self.best_params = None\n",
    "        self.study = None\n",
    "        \n",
    "    def create_features(self, df):\n",
    "        \"\"\"\n",
    "        Create advanced features for LSTM training\n",
    "        \"\"\"\n",
    "        # Create a copy to avoid modifying original\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Handle timezone-aware timestamps\n",
    "        if 'timestamp' in df.columns:\n",
    "            # Convert timezone-aware to timezone-naive UTC\n",
    "            if hasattr(df['timestamp'].iloc[0], 'tz'):\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_localize(None)\n",
    "            else:\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Core OHLCV features\n",
    "        features['open'] = df['open'].astype(float)\n",
    "        features['high'] = df['high'].astype(float)\n",
    "        features['low'] = df['low'].astype(float)\n",
    "        features['close'] = df['close'].astype(float)\n",
    "        features['volume'] = df['volume'].astype(float)\n",
    "        \n",
    "        # Price-based features\n",
    "        features['returns'] = df['close'].pct_change()\n",
    "        features['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "        features['high_low_pct'] = (df['high'] - df['low']) / (df['close'] + 1e-10)\n",
    "        features['close_open_pct'] = (df['close'] - df['open']) / (df['open'] + 1e-10)\n",
    "        \n",
    "        # Candlestick patterns\n",
    "        body_size = df['high'] - df['low'] + 1e-10\n",
    "        features['upper_shadow'] = (df['high'] - np.maximum(df['open'], df['close'])) / body_size\n",
    "        features['lower_shadow'] = (np.minimum(df['open'], df['close']) - df['low']) / body_size\n",
    "        features['body_size'] = np.abs(df['close'] - df['open']) / body_size\n",
    "        \n",
    "        # Volume features\n",
    "        volume_sma = df['volume'].rolling(20, min_periods=1).mean()\n",
    "        features['volume_sma_ratio'] = df['volume'] / (volume_sma + 1e-10)\n",
    "        volume_std = df['volume'].rolling(20, min_periods=1).std()\n",
    "        features['volume_std'] = volume_std / (volume_sma + 1e-10)\n",
    "        \n",
    "        if 'transactions' in df.columns:\n",
    "            features['transactions_per_volume'] = df['transactions'] / (df['volume'] + 1)\n",
    "        else:\n",
    "            features['transactions_per_volume'] = df['volume'] / 100000  # Estimate\n",
    "        \n",
    "        # Volatility features\n",
    "        features['volatility_5'] = features['returns'].rolling(5, min_periods=1).std()\n",
    "        features['volatility_20'] = features['returns'].rolling(20, min_periods=1).std()\n",
    "        features['volatility_ratio'] = features['volatility_5'] / (features['volatility_20'] + 1e-10)\n",
    "        \n",
    "        # Technical indicators\n",
    "        features['rsi'] = self.calculate_rsi(df['close'])\n",
    "        features['macd'], features['macd_signal'] = self.calculate_macd(df['close'])\n",
    "        features['bb_position'] = self.calculate_bollinger_position(df['close'])\n",
    "        features['stoch_k'], features['stoch_d'] = self.calculate_stochastic(df)\n",
    "        \n",
    "        # Moving averages\n",
    "        for period in [5, 10, 20]:  # Reduced to avoid too many features\n",
    "            sma = df['close'].rolling(period, min_periods=1).mean()\n",
    "            features[f'sma_{period}'] = sma\n",
    "            features[f'sma_{period}_ratio'] = df['close'] / (sma + 1e-10)\n",
    "        \n",
    "        # VIX features\n",
    "        if 'vix' in df.columns:\n",
    "            features['vix'] = df['vix'].astype(float)\n",
    "            features['vix_sma_20'] = df['vix'].rolling(20, min_periods=1).mean()\n",
    "            features['vix_ratio'] = df['vix'] / (features['vix_sma_20'] + 1e-10)\n",
    "        else:\n",
    "            features['vix'] = 20.0\n",
    "            features['vix_sma_20'] = 20.0\n",
    "            features['vix_ratio'] = 1.0\n",
    "        \n",
    "        # Sentiment\n",
    "        if 'sentiment_score' in df.columns:\n",
    "            features['sentiment'] = df['sentiment_score'].astype(float)\n",
    "            features['sentiment_sma'] = df['sentiment_score'].rolling(20, min_periods=1).mean()\n",
    "        else:\n",
    "            features['sentiment'] = 0.5\n",
    "            features['sentiment_sma'] = 0.5\n",
    "        \n",
    "        # Time-based features (important for intraday patterns)\n",
    "        if 'timestamp' in df.columns:\n",
    "            # Use the timezone-naive timestamp\n",
    "            ts = df['timestamp']\n",
    "            features['hour'] = ts.dt.hour\n",
    "            features['minute'] = ts.dt.minute\n",
    "            features['day_of_week'] = ts.dt.dayofweek\n",
    "            features['is_morning'] = (features['hour'] < 11).astype(int)\n",
    "            features['is_afternoon'] = (features['hour'] >= 14).astype(int)\n",
    "            \n",
    "            # Minutes from market open (9:30 AM)\n",
    "            features['minutes_from_open'] = (features['hour'] - 9) * 60 + (features['minute'] - 30)\n",
    "            features['minutes_from_open'] = features['minutes_from_open'].clip(0, 390)\n",
    "            features['minutes_to_close'] = 390 - features['minutes_from_open']\n",
    "            \n",
    "            # Normalize time features\n",
    "            features['hour_norm'] = features['hour'] / 24\n",
    "            features['minute_norm'] = features['minute'] / 60\n",
    "            features['day_of_week_norm'] = features['day_of_week'] / 6\n",
    "        else:\n",
    "            # Default time features\n",
    "            features['hour'] = 12\n",
    "            features['minute'] = 0\n",
    "            features['day_of_week'] = 2\n",
    "            features['is_morning'] = 0\n",
    "            features['is_afternoon'] = 0\n",
    "            features['minutes_from_open'] = 150\n",
    "            features['minutes_to_close'] = 240\n",
    "            features['hour_norm'] = 0.5\n",
    "            features['minute_norm'] = 0.0\n",
    "            features['day_of_week_norm'] = 0.33\n",
    "        \n",
    "        # Fill NaN values\n",
    "        features = features.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "        \n",
    "        # Replace infinities\n",
    "        features = features.replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_rsi(self, prices, period=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-10)\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi.fillna(50)  # Neutral RSI when not enough data\n",
    "    \n",
    "    def calculate_macd(self, prices, fast=12, slow=26, signal=9):\n",
    "        exp1 = prices.ewm(span=fast, adjust=False).mean()\n",
    "        exp2 = prices.ewm(span=slow, adjust=False).mean()\n",
    "        macd = exp1 - exp2\n",
    "        macd_signal = macd.ewm(span=signal, adjust=False).mean()\n",
    "        return macd.fillna(0), macd_signal.fillna(0)\n",
    "    \n",
    "    def calculate_bollinger_position(self, prices, period=20):\n",
    "        sma = prices.rolling(period, min_periods=1).mean()\n",
    "        std = prices.rolling(period, min_periods=1).std()\n",
    "        std = std.fillna(prices.std())  # Use overall std when not enough data\n",
    "        upper = sma + (2 * std)\n",
    "        lower = sma - (2 * std)\n",
    "        position = (prices - lower) / (upper - lower + 1e-10)\n",
    "        return position.fillna(0.5).clip(0, 1)\n",
    "    \n",
    "    def calculate_stochastic(self, df, k_period=14, d_period=3):\n",
    "        low_min = df['low'].rolling(k_period, min_periods=1).min()\n",
    "        high_max = df['high'].rolling(k_period, min_periods=1).max()\n",
    "        k_percent = 100 * ((df['close'] - low_min) / (high_max - low_min + 1e-10))\n",
    "        d_percent = k_percent.rolling(d_period, min_periods=1).mean()\n",
    "        return k_percent.fillna(50), d_percent.fillna(50)\n",
    "    \n",
    "    def prepare_sequences(self, features, target_cols=['open', 'high', 'low', 'close']):\n",
    "        \"\"\"\n",
    "        Prepare sequences for LSTM training with proper time series handling\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        # Ensure we have enough data\n",
    "        min_required = self.sequence_length + self.bars_per_day\n",
    "        if len(features) < min_required:\n",
    "            print(f\"⚠️ Insufficient data: {len(features)} bars, need {min_required}\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(self.sequence_length, len(features) - self.bars_per_day):\n",
    "            # Input sequence\n",
    "            sequence = features.iloc[i-self.sequence_length:i].values\n",
    "            \n",
    "            # Check for NaN or Inf in sequence\n",
    "            if np.any(np.isnan(sequence)) or np.any(np.isinf(sequence)):\n",
    "                continue\n",
    "            \n",
    "            X.append(sequence)\n",
    "            \n",
    "            # Target: next day's OHLC\n",
    "            next_day_data = features.iloc[i:i+self.bars_per_day]\n",
    "            if len(next_day_data) == self.bars_per_day:\n",
    "                ohlc = np.array([\n",
    "                    next_day_data['open'].iloc[0],\n",
    "                    next_day_data['high'].max(),\n",
    "                    next_day_data['low'].min(),\n",
    "                    next_day_data['close'].iloc[-1]\n",
    "                ])\n",
    "                \n",
    "                # Check for valid OHLC values\n",
    "                if np.all(np.isfinite(ohlc)) and ohlc[1] >= ohlc[2]:  # high >= low\n",
    "                    y.append(ohlc)\n",
    "                else:\n",
    "                    X.pop()  # Remove the corresponding input if target is invalid\n",
    "        \n",
    "        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "    \n",
    "    def create_model(self, trial, input_shape):\n",
    "        \"\"\"\n",
    "        Create model with Optuna trial parameters\n",
    "        \"\"\"\n",
    "        # Model architecture choices\n",
    "        model_type = trial.suggest_categorical('model_type', ['lstm', 'gru'])  # Removed bilstm for stability\n",
    "        n_layers = trial.suggest_int('n_layers', 2, 3)  # Reduced max layers\n",
    "        \n",
    "        # Regularization\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.4, step=0.05)\n",
    "        use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "        \n",
    "        # Optimization\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # First layer\n",
    "        first_units = trial.suggest_int('units_layer_0', 64, 256, step=32)\n",
    "        \n",
    "        if model_type == 'lstm':\n",
    "            model.add(LSTM(\n",
    "                first_units,\n",
    "                return_sequences=(n_layers > 1),\n",
    "                input_shape=input_shape\n",
    "            ))\n",
    "        else:  # gru\n",
    "            model.add(GRU(\n",
    "                first_units,\n",
    "                return_sequences=(n_layers > 1),\n",
    "                input_shape=input_shape\n",
    "            ))\n",
    "        \n",
    "        model.add(Dropout(dropout_rate))\n",
    "        if use_batch_norm and n_layers > 1:\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, n_layers):\n",
    "            units = trial.suggest_int(f'units_layer_{i}', 32, 128, step=32)\n",
    "            return_seq = (i < n_layers - 1)\n",
    "            \n",
    "            if model_type == 'lstm':\n",
    "                model.add(LSTM(units, return_sequences=return_seq))\n",
    "            else:\n",
    "                model.add(GRU(units, return_sequences=return_seq))\n",
    "            \n",
    "            model.add(Dropout(dropout_rate * 0.8))\n",
    "            if use_batch_norm and return_seq:\n",
    "                model.add(BatchNormalization())\n",
    "        \n",
    "        # Dense layers\n",
    "        dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "        model.add(Dense(dense_units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate * 0.5))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(4, activation='linear'))\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def objective(self, trial, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Objective function for Optuna optimization\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Clear previous models\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            # Create model\n",
    "            model = self.create_model(trial, (X_train.shape[1], X_train.shape[2]))\n",
    "            \n",
    "            # Training parameters\n",
    "            batch_size = trial.suggest_int('batch_size', 16, 64, step=16)\n",
    "            \n",
    "            # Callbacks\n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=50,  # Reduced for faster tuning\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Get best validation loss\n",
    "            val_loss = min(history.history['val_loss'])\n",
    "            \n",
    "            return val_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def tune(self, df, n_trials=100, n_jobs=1):\n",
    "        \"\"\"\n",
    "        Run hyperparameter tuning\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Advanced Hyperparameter Tuning\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        # Prepare features\n",
    "        print(\"Creating features...\")\n",
    "        features = self.create_features(df)\n",
    "        print(f\"Features shape: {features.shape}\")\n",
    "        \n",
    "        # Prepare sequences\n",
    "        print(\"Preparing sequences...\")\n",
    "        X, y = self.prepare_sequences(features)\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(\"❌ Insufficient data for tuning\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        val_split = 0.2\n",
    "        split_idx = int(len(X) * (1 - val_split))\n",
    "        \n",
    "        X_train = X[:split_idx]\n",
    "        y_train = y[:split_idx]\n",
    "        X_val = X[split_idx:]\n",
    "        y_val = y[split_idx:]\n",
    "        \n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler_X = RobustScaler()\n",
    "        scaler_y = RobustScaler()\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(\n",
    "            X_train.reshape(-1, X_train.shape[-1])\n",
    "        ).reshape(X_train.shape)\n",
    "        \n",
    "        X_val_scaled = scaler_X.transform(\n",
    "            X_val.reshape(-1, X_val.shape[-1])\n",
    "        ).reshape(X_val.shape)\n",
    "        \n",
    "        y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "        y_val_scaled = scaler_y.transform(y_val)\n",
    "        \n",
    "        # Save scalers\n",
    "        joblib.dump(scaler_X, 'scaler_features.pkl')\n",
    "        joblib.dump(scaler_y, 'scaler_target.pkl')\n",
    "        \n",
    "        # Create study\n",
    "        self.study = optuna.create_study(\n",
    "            direction='minimize',\n",
    "            study_name='tesla_ohlc_tuning'\n",
    "        )\n",
    "        \n",
    "        # Optimize\n",
    "        print(f\"\\nRunning {n_trials} trials...\")\n",
    "        self.study.optimize(\n",
    "            lambda trial: self.objective(\n",
    "                trial, \n",
    "                X_train_scaled, y_train_scaled,\n",
    "                X_val_scaled, y_val_scaled\n",
    "            ),\n",
    "            n_trials=n_trials,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        \n",
    "        # Get best parameters\n",
    "        self.best_params = self.study.best_params\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Best Hyperparameters Found\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for key, value in self.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\nBest validation loss: {self.study.best_value:.6f}\")\n",
    "        \n",
    "        # Save results\n",
    "        with open('best_hyperparameters.json', 'w') as f:\n",
    "            json.dump(self.best_params, f, indent=4)\n",
    "        \n",
    "        print(f\"\\n✅ Best parameters saved to best_hyperparameters.json\")\n",
    "        \n",
    "        return self.best_params\n",
    "\n",
    "# Main function\n",
    "def run_hyperparameter_tuning(df=None, n_trials=100):\n",
    "    \"\"\"\n",
    "    Main function to run hyperparameter tuning\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" HYPERPARAMETER TUNING FOR TESLA OHLC PREDICTION \".center(80, \"=\"))\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Load data if not provided\n",
    "    if df is None:\n",
    "        print(\"Loading data...\")\n",
    "        try:\n",
    "            df = pd.read_pickle('tesla_6months_5min_data.pkl')\n",
    "            print(f\"✅ Loaded {len(df)} bars from saved data\")\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv('tesla_6months_5min_data.csv')\n",
    "                print(f\"✅ Loaded {len(df)} bars from CSV\")\n",
    "            except:\n",
    "                print(\"❌ No saved data found. Please run data collection first.\")\n",
    "                return None\n",
    "    \n",
    "    # Initialize tuner\n",
    "    tuner = AdvancedHyperparameterTuner(\n",
    "        lookback_days=20,\n",
    "        interval_minutes=5\n",
    "    )\n",
    "    \n",
    "    # Run tuning\n",
    "    best_params = tuner.tune(df, n_trials=n_trials)\n",
    "    \n",
    "    if best_params:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" TUNING COMPLETE \".center(80, \"=\"))\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run with fewer trials for testing\n",
    "    best_params = run_hyperparameter_tuning(n_trials=20)  # Reduced for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a69fe81",
   "metadata": {},
   "source": [
    "# Load your 6 months of data\n",
    "df = pd.read_pickle('tesla_6months_5min_data.pkl')\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "best_params = run_hyperparameter_tuning(df, n_trials=100)\n",
    "\n",
    "# Results are saved to:\n",
    "# - best_hyperparameters.json (best parameters)\n",
    "# - optuna_study.pkl (full study object)\n",
    "# - scaler_features.pkl (feature scaler)\n",
    "# - scaler_target.pkl (target scaler)\n",
    "# - hyperparameter_tuning_results.png (visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your 6 months of data\n",
    "df = pd.read_pickle('tesla_6months_5min_data.pkl')\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "best_params = run_hyperparameter_tuning(df, n_trials=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93167feb",
   "metadata": {},
   "source": [
    "train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7100ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "=============== HYPERPARAMETER TUNING FOR TESLA OHLC PREDICTION ================\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "✅ Loaded 3267 bars from saved data\n",
      "\n",
      "============================================================\n",
      "Advanced Hyperparameter Tuning\n",
      "============================================================\n",
      "\n",
      "Creating features...\n",
      "Features shape: (3267, 35)\n",
      "Preparing sequences...\n",
      "X shape: (1629, 1560, 35), y shape: (1629, 4)\n",
      "Training samples: 1303\n",
      "Validation samples: 326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 19:23:31,129] A new study created in memory with name: tesla_ohlc_tuning\n",
      "2025-11-09 19:23:31.315029: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 20 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:23:31.883258: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 284575200 exceeds 10% of free system memory.\n",
      "2025-11-09 19:23:37.054045: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 38338560 exceeds 10% of free system memory.\n",
      "2025-11-09 19:23:37.076264: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 38338560 exceeds 10% of free system memory.\n",
      "2025-11-09 19:23:37.101858: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 38338560 exceeds 10% of free system memory.\n",
      "2025-11-09 19:23:37.204474: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 38338560 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AdvancedHyperparameterTuner:\n",
    "    \"\"\"\n",
    "    Advanced hyperparameter tuning for OHLC prediction with 6 months of data\n",
    "    \"\"\"\n",
    "    def __init__(self, lookback_days=20, interval_minutes=5):\n",
    "        self.lookback_days = lookback_days\n",
    "        self.interval_minutes = interval_minutes\n",
    "        self.bars_per_day = 390 // interval_minutes  # 78 for 5-min bars\n",
    "        self.sequence_length = self.bars_per_day * lookback_days\n",
    "        self.best_params = None\n",
    "        self.study = None\n",
    "        \n",
    "    def create_features(self, df):\n",
    "        \"\"\"\n",
    "        Create advanced features for LSTM training\n",
    "        \"\"\"\n",
    "        # Create a copy to avoid modifying original\n",
    "        df = df.copy()\n",
    "        \n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Core OHLCV features\n",
    "        features['open'] = df['open'].astype(float)\n",
    "        features['high'] = df['high'].astype(float)\n",
    "        features['low'] = df['low'].astype(float)\n",
    "        features['close'] = df['close'].astype(float)\n",
    "        features['volume'] = df['volume'].astype(float)\n",
    "        \n",
    "        # Price-based features\n",
    "        features['returns'] = df['close'].pct_change()\n",
    "        features['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "        features['high_low_pct'] = (df['high'] - df['low']) / (df['close'] + 1e-10)\n",
    "        features['close_open_pct'] = (df['close'] - df['open']) / (df['open'] + 1e-10)\n",
    "        \n",
    "        # Candlestick patterns\n",
    "        body_size = df['high'] - df['low'] + 1e-10\n",
    "        features['upper_shadow'] = (df['high'] - np.maximum(df['open'], df['close'])) / body_size\n",
    "        features['lower_shadow'] = (np.minimum(df['open'], df['close']) - df['low']) / body_size\n",
    "        features['body_size'] = np.abs(df['close'] - df['open']) / body_size\n",
    "        \n",
    "        # Volume features\n",
    "        volume_sma = df['volume'].rolling(20, min_periods=1).mean()\n",
    "        features['volume_sma_ratio'] = df['volume'] / (volume_sma + 1e-10)\n",
    "        volume_std = df['volume'].rolling(20, min_periods=1).std()\n",
    "        features['volume_std'] = volume_std / (volume_sma + 1e-10)\n",
    "        \n",
    "        if 'transactions' in df.columns:\n",
    "            features['transactions_per_volume'] = df['transactions'] / (df['volume'] + 1)\n",
    "        else:\n",
    "            features['transactions_per_volume'] = df['volume'] / 100000  # Estimate\n",
    "        \n",
    "        # Volatility features\n",
    "        features['volatility_5'] = features['returns'].rolling(5, min_periods=1).std()\n",
    "        features['volatility_20'] = features['returns'].rolling(20, min_periods=1).std()\n",
    "        features['volatility_ratio'] = features['volatility_5'] / (features['volatility_20'] + 1e-10)\n",
    "        \n",
    "        # Technical indicators\n",
    "        features['rsi'] = self.calculate_rsi(df['close'])\n",
    "        features['macd'], features['macd_signal'] = self.calculate_macd(df['close'])\n",
    "        features['bb_position'] = self.calculate_bollinger_position(df['close'])\n",
    "        features['stoch_k'], features['stoch_d'] = self.calculate_stochastic(df)\n",
    "        \n",
    "        # Moving averages\n",
    "        for period in [5, 10, 20]:  # Reduced to avoid too many features\n",
    "            sma = df['close'].rolling(period, min_periods=1).mean()\n",
    "            features[f'sma_{period}'] = sma\n",
    "            features[f'sma_{period}_ratio'] = df['close'] / (sma + 1e-10)\n",
    "        \n",
    "        # VIX features\n",
    "        if 'vix' in df.columns:\n",
    "            features['vix'] = df['vix'].astype(float)\n",
    "            features['vix_sma_20'] = df['vix'].rolling(20, min_periods=1).mean()\n",
    "            features['vix_ratio'] = df['vix'] / (features['vix_sma_20'] + 1e-10)\n",
    "        else:\n",
    "            features['vix'] = 20.0\n",
    "            features['vix_sma_20'] = 20.0\n",
    "            features['vix_ratio'] = 1.0\n",
    "        \n",
    "        # Sentiment\n",
    "        if 'sentiment_score' in df.columns:\n",
    "            features['sentiment'] = df['sentiment_score'].astype(float)\n",
    "            features['sentiment_sma'] = df['sentiment_score'].rolling(20, min_periods=1).mean()\n",
    "        else:\n",
    "            features['sentiment'] = 0.5\n",
    "            features['sentiment_sma'] = 0.5\n",
    "        \n",
    "        # Time-based features (important for intraday patterns)\n",
    "        ##################################\n",
    "        # Normalize timestamps: handle strings, python datetimes (aware/naive), mixed types\n",
    "        if 'timestamp' in df.columns:\n",
    "    # 1) Parse everything as tz-aware UTC to avoid mixed-type errors\n",
    "            ts = pd.to_datetime(df['timestamp'], errors='coerce', utc=True)\n",
    "\n",
    "    # 2) (Optional but recommended here) convert to US/Eastern because your intraday\n",
    "    #    features assume the US market clock (9:30–16:00 ET)\n",
    "            ts = ts.dt.tz_convert('America/New_York')\n",
    "\n",
    "    # 3) Drop timezone to make downstream math simple\n",
    "            df['timestamp'] = ts.dt.tz_localize(None)\n",
    "\n",
    "        ##################################\n",
    "        # Fill NaN values\n",
    "        features = features.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "        \n",
    "        # Replace infinities\n",
    "        features = features.replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_rsi(self, prices, period=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-10)\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi.fillna(50)  # Neutral RSI when not enough data\n",
    "    \n",
    "    def calculate_macd(self, prices, fast=12, slow=26, signal=9):\n",
    "        exp1 = prices.ewm(span=fast, adjust=False).mean()\n",
    "        exp2 = prices.ewm(span=slow, adjust=False).mean()\n",
    "        macd = exp1 - exp2\n",
    "        macd_signal = macd.ewm(span=signal, adjust=False).mean()\n",
    "        return macd.fillna(0), macd_signal.fillna(0)\n",
    "    \n",
    "    def calculate_bollinger_position(self, prices, period=20):\n",
    "        sma = prices.rolling(period, min_periods=1).mean()\n",
    "        std = prices.rolling(period, min_periods=1).std()\n",
    "        std = std.fillna(prices.std())  # Use overall std when not enough data\n",
    "        upper = sma + (2 * std)\n",
    "        lower = sma - (2 * std)\n",
    "        position = (prices - lower) / (upper - lower + 1e-10)\n",
    "        return position.fillna(0.5).clip(0, 1)\n",
    "    \n",
    "    def calculate_stochastic(self, df, k_period=14, d_period=3):\n",
    "        low_min = df['low'].rolling(k_period, min_periods=1).min()\n",
    "        high_max = df['high'].rolling(k_period, min_periods=1).max()\n",
    "        k_percent = 100 * ((df['close'] - low_min) / (high_max - low_min + 1e-10))\n",
    "        d_percent = k_percent.rolling(d_period, min_periods=1).mean()\n",
    "        return k_percent.fillna(50), d_percent.fillna(50)\n",
    "    \n",
    "    def prepare_sequences(self, features, target_cols=['open', 'high', 'low', 'close']):\n",
    "        \"\"\"\n",
    "        Prepare sequences for LSTM training with proper time series handling\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        # Ensure we have enough data\n",
    "        min_required = self.sequence_length + self.bars_per_day\n",
    "        if len(features) < min_required:\n",
    "            print(f\"⚠️ Insufficient data: {len(features)} bars, need {min_required}\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(self.sequence_length, len(features) - self.bars_per_day):\n",
    "            # Input sequence\n",
    "            sequence = features.iloc[i-self.sequence_length:i].values\n",
    "            \n",
    "            # Check for NaN or Inf in sequence\n",
    "            if np.any(np.isnan(sequence)) or np.any(np.isinf(sequence)):\n",
    "                continue\n",
    "            \n",
    "            X.append(sequence)\n",
    "            \n",
    "            # Target: next day's OHLC\n",
    "            next_day_data = features.iloc[i:i+self.bars_per_day]\n",
    "            if len(next_day_data) == self.bars_per_day:\n",
    "                ohlc = np.array([\n",
    "                    next_day_data['open'].iloc[0],\n",
    "                    next_day_data['high'].max(),\n",
    "                    next_day_data['low'].min(),\n",
    "                    next_day_data['close'].iloc[-1]\n",
    "                ])\n",
    "                \n",
    "                # Check for valid OHLC values\n",
    "                if np.all(np.isfinite(ohlc)) and ohlc[1] >= ohlc[2]:  # high >= low\n",
    "                    y.append(ohlc)\n",
    "                else:\n",
    "                    X.pop()  # Remove the corresponding input if target is invalid\n",
    "        \n",
    "        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "    \n",
    "    def create_model(self, trial, input_shape):\n",
    "        \"\"\"\n",
    "        Create model with Optuna trial parameters\n",
    "        \"\"\"\n",
    "        # Model architecture choices\n",
    "        model_type = trial.suggest_categorical('model_type', ['lstm', 'gru'])  # Removed bilstm for stability\n",
    "        n_layers = trial.suggest_int('n_layers', 2, 3)  # Reduced max layers\n",
    "        \n",
    "        # Regularization\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.4, step=0.05)\n",
    "        use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "        \n",
    "        # Optimization\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # First layer\n",
    "        first_units = trial.suggest_int('units_layer_0', 64, 256, step=32)\n",
    "        \n",
    "        if model_type == 'lstm':\n",
    "            model.add(LSTM(\n",
    "                first_units,\n",
    "                return_sequences=(n_layers > 1),\n",
    "                input_shape=input_shape\n",
    "            ))\n",
    "        else:  # gru\n",
    "            model.add(GRU(\n",
    "                first_units,\n",
    "                return_sequences=(n_layers > 1),\n",
    "                input_shape=input_shape\n",
    "            ))\n",
    "        \n",
    "        model.add(Dropout(dropout_rate))\n",
    "        if use_batch_norm and n_layers > 1:\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, n_layers):\n",
    "            units = trial.suggest_int(f'units_layer_{i}', 32, 128, step=32)\n",
    "            return_seq = (i < n_layers - 1)\n",
    "            \n",
    "            if model_type == 'lstm':\n",
    "                model.add(LSTM(units, return_sequences=return_seq))\n",
    "            else:\n",
    "                model.add(GRU(units, return_sequences=return_seq))\n",
    "            \n",
    "            model.add(Dropout(dropout_rate * 0.8))\n",
    "            if use_batch_norm and return_seq:\n",
    "                model.add(BatchNormalization())\n",
    "        \n",
    "        # Dense layers\n",
    "        dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "        model.add(Dense(dense_units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate * 0.5))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(4, activation='linear'))\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def objective(self, trial, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Objective function for Optuna optimization\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Clear previous models\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            # Create model\n",
    "            model = self.create_model(trial, (X_train.shape[1], X_train.shape[2]))\n",
    "            \n",
    "            # Training parameters\n",
    "            batch_size = trial.suggest_int('batch_size', 16, 64, step=16)\n",
    "            \n",
    "            # Callbacks\n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=50,  # Reduced for faster tuning\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Get best validation loss\n",
    "            val_loss = min(history.history['val_loss'])\n",
    "            \n",
    "            return val_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def tune(self, df, n_trials=100, n_jobs=1):\n",
    "        \"\"\"\n",
    "        Run hyperparameter tuning\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Advanced Hyperparameter Tuning\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        # Prepare features\n",
    "        print(\"Creating features...\")\n",
    "        features = self.create_features(df)\n",
    "        print(f\"Features shape: {features.shape}\")\n",
    "        \n",
    "        # Prepare sequences\n",
    "        print(\"Preparing sequences...\")\n",
    "        X, y = self.prepare_sequences(features)\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(\"❌ Insufficient data for tuning\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        val_split = 0.2\n",
    "        split_idx = int(len(X) * (1 - val_split))\n",
    "        \n",
    "        X_train = X[:split_idx]\n",
    "        y_train = y[:split_idx]\n",
    "        X_val = X[split_idx:]\n",
    "        y_val = y[split_idx:]\n",
    "        \n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler_X = RobustScaler()\n",
    "        scaler_y = RobustScaler()\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(\n",
    "            X_train.reshape(-1, X_train.shape[-1])\n",
    "        ).reshape(X_train.shape)\n",
    "        \n",
    "        X_val_scaled = scaler_X.transform(\n",
    "            X_val.reshape(-1, X_val.shape[-1])\n",
    "        ).reshape(X_val.shape)\n",
    "        \n",
    "        y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "        y_val_scaled = scaler_y.transform(y_val)\n",
    "        \n",
    "        # Save scalers\n",
    "        joblib.dump(scaler_X, 'scaler_features.pkl')\n",
    "        joblib.dump(scaler_y, 'scaler_target.pkl')\n",
    "        \n",
    "        # Create study\n",
    "        self.study = optuna.create_study(\n",
    "            direction='minimize',\n",
    "            study_name='tesla_ohlc_tuning'\n",
    "        )\n",
    "        \n",
    "        # Optimize\n",
    "        print(f\"\\nRunning {n_trials} trials...\")\n",
    "        self.study.optimize(\n",
    "            lambda trial: self.objective(\n",
    "                trial, \n",
    "                X_train_scaled, y_train_scaled,\n",
    "                X_val_scaled, y_val_scaled\n",
    "            ),\n",
    "            n_trials=n_trials,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        \n",
    "        # Get best parameters\n",
    "        self.best_params = self.study.best_params\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Best Hyperparameters Found\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for key, value in self.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\nBest validation loss: {self.study.best_value:.6f}\")\n",
    "        \n",
    "        # Save results\n",
    "        with open('best_hyperparameters.json', 'w') as f:\n",
    "            json.dump(self.best_params, f, indent=4)\n",
    "        \n",
    "        print(f\"\\n✅ Best parameters saved to best_hyperparameters.json\")\n",
    "        \n",
    "        return self.best_params\n",
    "\n",
    "# Main function\n",
    "def run_hyperparameter_tuning(df=None, n_trials=100):\n",
    "    \"\"\"\n",
    "    Main function to run hyperparameter tuning\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" HYPERPARAMETER TUNING FOR TESLA OHLC PREDICTION \".center(80, \"=\"))\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Load data if not provided\n",
    "    if df is None:\n",
    "        print(\"Loading data...\")\n",
    "        try:\n",
    "            df = pd.read_pickle('tesla_6months_5min_data.pkl')\n",
    "            print(f\"✅ Loaded {len(df)} bars from saved data\")\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv('tesla_6months_5min_data.csv')\n",
    "                print(f\"✅ Loaded {len(df)} bars from CSV\")\n",
    "            except:\n",
    "                print(\"❌ No saved data found. Please run data collection first.\")\n",
    "                return None\n",
    "    \n",
    "    # Initialize tuner\n",
    "    tuner = AdvancedHyperparameterTuner(\n",
    "        lookback_days=20,\n",
    "        interval_minutes=5\n",
    "    )\n",
    "    \n",
    "    # Run tuning\n",
    "    best_params = tuner.tune(df, n_trials=n_trials)\n",
    "    \n",
    "    if best_params:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" TUNING COMPLETE \".center(80, \"=\"))\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run with fewer trials for testing\n",
    "    best_params = run_hyperparameter_tuning(n_trials=20)  # Reduced for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b1af58",
   "metadata": {},
   "source": [
    "# Run the complete pipeline\n",
    "main_pipeline()\n",
    "\n",
    "# Or run blocks separately:\n",
    "\n",
    "# 1. Just collect data\n",
    "df = collect_data()\n",
    "\n",
    "# 2. Just tune hyperparameters (needs data)\n",
    "trainer = TeslaOHLCTrainer()\n",
    "X, y = trainer.prepare_data(df)\n",
    "best_params = tune_hyperparameters(X, y)\n",
    "\n",
    "# 3. Just train and predict (needs data and params)\n",
    "trainer.train(X, y, best_params)\n",
    "prediction = trainer.predict_next_day(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1622d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "================= LOADING 6 MONTHS OF 5-MINUTE DATA FOR TESLA ==================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fetching 6 Months of 5-Minute Bar Data\n",
      "============================================================\n",
      "\n",
      "📁 Loading from cache...\n",
      "✅ Loaded 3267 bars from cache\n",
      "\n",
      "Adding market features...\n",
      "✅ Added market features\n",
      "\n",
      "Validating data...\n",
      "\n",
      "📊 Data Statistics:\n",
      "  Date range: 2025-09-11 to 2025-11-07\n",
      "  Total bars: 3,267\n",
      "  Trading days: ~57\n",
      "  Bars per day: ~57\n",
      "  Expected (78 bars/day): 4,446\n",
      "\n",
      "✅ Data saved to tesla_6months_5min_data.csv\n",
      "✅ Data saved to tesla_6months_5min_data.pkl\n",
      "\n",
      "📈 Final Dataset:\n",
      "  Shape: (3267, 11)\n",
      "  Memory usage: 0.27 MB\n",
      "  Columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'dividends', 'stock splits', 'vix', 'sentiment_score', 'transactions']\n",
      "\n",
      "🔮 For LSTM Training:\n",
      "  Lookback period: 20 days\n",
      "  Sequence length: 1560 bars\n",
      "  Potential training sequences: ~21\n",
      "  ⚠️ Warning: May need more data for robust training\n"
     ]
    }
   ],
   "source": [
    "df = load_6months_5min_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
